import argparse
import os
import json
import time
import pandas as pd
import numpy as np
import requests
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, f1_score
import matplotlib.pyplot as plt
import seaborn as sns

# -------------------------- æœ¬åœ°Ollamaæ ¸å¿ƒé…ç½®ï¼ˆéœ€æ ¹æ®ä½ çš„ç¯å¢ƒç¡®è®¤ï¼‰ --------------------------
OLLAMA_BASE_URL = "http://127.0.0.1:11434"  # Ollamaé»˜è®¤æœ¬åœ°æœåŠ¡ç«¯å£ï¼ˆå›ºå®šï¼‰
OLLAMA_BASE_MODEL = "llama3:8b"  # æœ¬åœ°åŸºç¡€æ¨¡å‹ï¼ˆéœ€å…ˆé€šè¿‡ `ollama pull llama3:8b` ä¸‹è½½ï¼‰
FINETUNED_MODEL_NAME = "llama3:8b-model-compare-finetuned"  # å¾®è°ƒåæ¨¡å‹çš„è‡ªå®šä¹‰åç§°
DATA_FILE = "train.csv"  # æœ¬åœ°æ•°æ®æ–‡ä»¶ï¼ˆéœ€ä¸ä»£ç åŒç›®å½•ï¼Œæˆ–å†™ç»å¯¹è·¯å¾„ï¼‰
LABEL_MAPPING = {0: "Aæ›´å¥½", 1: "Bæ›´å¥½", 2: "ä¸€æ ·å¥½"}  # æ ‡ç­¾æ–‡æœ¬æ˜ å°„
LABEL_TO_OLLAMA = {0: "A", 1: "B", 2: "SAME"}  # å¾®è°ƒæ—¶ä¼ ç»™Ollamaçš„ç®€æ´æ ‡ç­¾

# -------------------------- 1. æ•°æ®å¤„ç†å·¥å…·å‡½æ•° --------------------------
def read_data(path):
    """è¯»å–CSV/JSONLæ•°æ®ï¼Œå…¼å®¹ä¸¤ç§æ ¼å¼"""
    if path.endswith(".csv"):
        return pd.read_csv(path, encoding="utf-8")
    if path.endswith(".jsonl") or path.endswith(".json"):
        return pd.read_json(path, lines=True, encoding="utf-8")
    raise ValueError("ä»…æ”¯æŒ CSV æˆ– JSONL/JSON æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥æ•°æ®æ ¼å¼")

def truthy(value):
    """åˆ¤æ–­å€¼æ˜¯å¦ä¸ºâ€œçœŸâ€ï¼ˆç”¨äºè§£ææ ‡æ³¨çš„winneræ ‡ç­¾ï¼‰"""
    if pd.isna(value):
        return False
    if isinstance(value, (int, float, np.integer, np.floating)):
        return int(value) != 0
    value_str = str(value).strip().lower()
    return value_str in ("1", "true", "yes", "y", "t", "1.0")

def infer_true_label(row):
    """ä»æ•°æ®è¡Œä¸­æå–çœŸå®æ ‡ç­¾ï¼ˆ0=Aæ›´å¥½ï¼Œ1=Bæ›´å¥½ï¼Œ2=ä¸€æ ·å¥½ï¼‰"""
    # ä¼˜å…ˆè§£æå¤šåˆ—æ ‡æ³¨ï¼ˆwinner_model_a/winner_model_b/winner_tieï¼‰
    if truthy(row.get("winner_model_a", None)):
        return 0
    if truthy(row.get("winner_model_b", None)):
        return 1
    if truthy(row.get("winner_tie", None)):
        return 2
    # å…¼å®¹å•åˆ—æ ‡æ³¨ï¼ˆä»…winneråˆ—ï¼‰
    winner_col = row.get("winner", None)
    if pd.notna(winner_col):
        winner_str = str(winner_col).strip().lower()
        if winner_str in ("a", "model_a", "model a", "winner a"):
            return 0
        if winner_str in ("b", "model_b", "model b", "winner b"):
            return 1
        if winner_str in ("tie", "same", "equal", "both"):
            return 2
    # è‹¥æ— æ³•è§£æï¼ŒæŠ›å‡ºé”™è¯¯ï¼ˆéœ€æ£€æŸ¥æ•°æ®æ ‡æ³¨æ ¼å¼ï¼‰
    raise ValueError(f"æ— æ³•è§£ææ ‡ç­¾ï¼è¡ŒIDï¼š{row.get('id', 'æœªçŸ¥')}ï¼Œè¯·æ£€æŸ¥æ ‡æ³¨åˆ—")

def build_ollama_finetune_sample(row):
    """æ„å»ºOllamaå¾®è°ƒè¦æ±‚çš„æ ·æœ¬æ ¼å¼ï¼ˆJSONLï¼š{"prompt": "...", "response": "..."}ï¼‰"""
    prompt = str(row.get("prompt", "")).strip()
    response_a = str(row.get("response_a", "")).strip()
    response_b = str(row.get("response_b", "")).strip()
    true_label = row["true_label"]  # å·²é€šè¿‡infer_true_labelè·å–
    
    # å¾®è°ƒæç¤ºè¯ï¼ˆæ˜ç¡®ä»»åŠ¡ï¼Œè®©æ¨¡å‹å­¦ä¹ åˆ¤æ–­é€»è¾‘ï¼‰
    finetune_prompt = f"""ä»»åŠ¡ï¼šå¯¹æ¯”ä¸¤ä¸ªæ¨¡å‹çš„å›ç­”ï¼Œåˆ¤æ–­å“ªä¸ªæ›´ç¬¦åˆç”¨æˆ·æç¤ºã€‚
ç”¨æˆ·æç¤ºï¼š{prompt}
æ¨¡å‹Açš„å›ç­”ï¼š{response_a}
æ¨¡å‹Bçš„å›ç­”ï¼š{response_b}
è¦æ±‚ï¼šä»…è¾“å‡ºæœ€ç»ˆåˆ¤æ–­ç»“æœï¼ˆA=Aæ›´å¥½ï¼ŒB=Bæ›´å¥½ï¼ŒSAME=ä¸¤è€…ä¸€æ ·å¥½ï¼‰ï¼Œä¸è¦é¢å¤–æ–‡å­—ã€‚
åˆ¤æ–­ç»“æœï¼š"""
    
    # Ollamaå¾®è°ƒè¦æ±‚çš„é”®å¿…é¡»æ˜¯"prompt"å’Œ"response"
    return {
        "prompt": finetune_prompt,
        "response": LABEL_TO_OLLAMA[true_label]
    }

def save_finetune_data(samples, save_path):
    """å°†å¾®è°ƒæ ·æœ¬ä¿å­˜ä¸ºOllamaæ”¯æŒçš„JSONLæ ¼å¼"""
    with open(save_path, "w", encoding="utf-8") as f:
        for sample in samples:
            f.write(json.dumps(sample, ensure_ascii=False) + "\n")
    print(f"âœ… å¾®è°ƒæ•°æ®å·²ä¿å­˜åˆ°ï¼š{save_path}")
    return save_path

# -------------------------- 2. OllamaæœåŠ¡äº¤äº’å‡½æ•° --------------------------
def check_ollama_service():
    """æ£€æŸ¥æœ¬åœ°OllamaæœåŠ¡æ˜¯å¦å·²å¯åŠ¨ï¼ˆå¿…é¡»å…ˆå¯åŠ¨æœåŠ¡æ‰èƒ½è¿è¡Œï¼‰"""
    try:
        response = requests.get(f"{OLLAMA_BASE_URL}/api/tags", timeout=5)
        if response.status_code == 200:
            print("âœ… æœ¬åœ°OllamaæœåŠ¡å·²æ­£å¸¸å¯åŠ¨")
            return True
        else:
            print(f"âŒ OllamaæœåŠ¡å“åº”å¼‚å¸¸ï¼ŒçŠ¶æ€ç ï¼š{response.status_code}")
            return False
    except requests.exceptions.ConnectionError:
        print("âŒ æœªæ£€æµ‹åˆ°OllamaæœåŠ¡ï¼è¯·å…ˆå¯åŠ¨æœåŠ¡ï¼š")
        print("  1. æ‰“å¼€CMDå‘½ä»¤æç¤ºç¬¦")
        print("  2. è¾“å…¥å‘½ä»¤ï¼šollama serveï¼ˆä¸è¦å…³é—­æ­¤CMDçª—å£ï¼‰")
        print("  3. é‡æ–°è¿è¡Œæœ¬ä»£ç ")
        return False

def pull_ollama_base_model(model_name):
    """æ‹‰å–åŸºç¡€æ¨¡å‹ï¼ˆå¦‚æœæœ¬åœ°æ²¡æœ‰ï¼‰ï¼Œç¡®ä¿å¾®è°ƒèƒ½æ­£å¸¸è¿›è¡Œ"""
    print(f"\nğŸ” æ£€æŸ¥æœ¬åœ°æ˜¯å¦å­˜åœ¨åŸºç¡€æ¨¡å‹ï¼š{model_name}")
    # å…ˆæŸ¥è¯¢æœ¬åœ°å·²æœ‰çš„æ¨¡å‹
    try:
        response = requests.get(f"{OLLAMA_BASE_URL}/api/tags", timeout=10)
        local_models = [m["name"] for m in response.json().get("models", [])]
        if model_name in local_models:
            print(f"âœ… åŸºç¡€æ¨¡å‹ {model_name} å·²å­˜åœ¨äºæœ¬åœ°")
            return True
    except Exception as e:
        print(f"âš ï¸ æŸ¥è¯¢æœ¬åœ°æ¨¡å‹æ—¶å‡ºé”™ï¼š{str(e)}ï¼Œå°†å°è¯•ç›´æ¥æ‹‰å–")
    
    # æœ¬åœ°æ²¡æœ‰æ¨¡å‹ï¼Œå¼€å§‹æ‹‰å–ï¼ˆå¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼Œå–å†³äºç½‘ç»œå’Œæ¨¡å‹å¤§å°ï¼‰
    print(f"ğŸ“¥ å¼€å§‹æ‹‰å–åŸºç¡€æ¨¡å‹ {model_name}ï¼ˆè¯·å‹¿ä¸­æ–­ï¼‰...")
    payload = {"name": model_name}
    try:
        # Ollamaæ‹‰å–æ˜¯æµå¼å“åº”ï¼Œå®æ—¶æ‰“å°è¿›åº¦
        with requests.post(
            f"{OLLAMA_BASE_URL}/api/pull", 
            json=payload, 
            stream=True, 
            timeout=3600  # è¶…æ—¶è®¾ä¸º1å°æ—¶ï¼Œé¿å…å¤§æ¨¡å‹æ‹‰å–ä¸­æ–­
        ) as response:
            for line in response.iter_lines():
                if line:
                    data = json.loads(line.decode("utf-8"))
                    # æ‰“å°å…³é”®çŠ¶æ€ï¼ˆå¦‚ä¸‹è½½è¿›åº¦ã€è§£å‹çŠ¶æ€ï¼‰
                    if "status" in data:
                        print(f"  æ‹‰å–çŠ¶æ€ï¼š{data['status']}", end="\r")
                    # æ‹‰å–å®Œæˆçš„æ ‡å¿—
                    if "completed" in data and data["completed"]:
                        print(f"\nâœ… åŸºç¡€æ¨¡å‹ {model_name} æ‹‰å–å®Œæˆ")
                        return True
        print(f"âŒ æ¨¡å‹æ‹‰å–æœªå®Œæˆï¼ˆæœªçŸ¥åŸå› ï¼‰")
        return False
    except Exception as e:
        print(f"âŒ æ‹‰å–æ¨¡å‹æ—¶å‡ºé”™ï¼š{str(e)}ï¼Œè¯·æ£€æŸ¥ç½‘ç»œåé‡è¯•")
        return False

def finetune_with_ollama(base_model, finetuned_model, data_path):
    """è°ƒç”¨Ollama APIè¿›è¡Œæ¨¡å‹å¾®è°ƒï¼ˆæ ¸å¿ƒæ­¥éª¤ï¼Œä½¿ç”¨æœ¬åœ°GPUåŠ é€Ÿï¼‰"""
    print(f"\nğŸš€ å¼€å§‹å¾®è°ƒæ¨¡å‹ï¼š")
    print(f"  åŸºç¡€æ¨¡å‹ï¼š{base_model}")
    print(f"  å¾®è°ƒåæ¨¡å‹åï¼š{finetuned_model}")
    print(f"  å¾®è°ƒæ•°æ®è·¯å¾„ï¼š{data_path}")
    
    # Ollamaå¾®è°ƒå‚æ•°ï¼ˆæ ¹æ®GPUå†…å­˜è°ƒæ•´ï¼Œå†…å­˜å°åˆ™å‡å°batch_sizeï¼‰
    finetune_payload = {
        "base_model": base_model,
        "name": finetuned_model,
        "data": data_path,
        "options": {
            "gpu": True,  # å¼ºåˆ¶å¯ç”¨GPUåŠ é€Ÿï¼ˆOllamaä¼šè‡ªåŠ¨è¯†åˆ«æœ¬åœ°GPUï¼‰
            "batch_size": 2,  # æ‰¹æ¬¡å¤§å°ï¼ˆ16GBå†…å­˜å»ºè®®2-4ï¼Œ32GBå»ºè®®4-8ï¼‰
            "epochs": 3,  # è®­ç»ƒè½®æ•°ï¼ˆæ•°æ®é‡å°æ—¶å¯å¢åŠ åˆ°5ï¼Œé¿å…è¿‡æ‹Ÿåˆï¼‰
            "learning_rate": 1e-4  # å­¦ä¹ ç‡ï¼ˆOllamaé»˜è®¤1e-4ï¼Œæ— éœ€é¢‘ç¹ä¿®æ”¹ï¼‰
        }
    }
    
    try:
        # å‘é€å¾®è°ƒè¯·æ±‚ï¼ˆæµå¼å“åº”ï¼Œå®æ—¶æ‰“å°è®­ç»ƒæ—¥å¿—ï¼‰
        with requests.post(
            f"{OLLAMA_BASE_URL}/api/finetune", 
            json=finetune_payload, 
            stream=True, 
            timeout=3600  # å¾®è°ƒè¶…æ—¶è®¾ä¸º1å°æ—¶
        ) as response:
            if response.status_code != 200:
                error_msg = response.json().get("error", "æœªçŸ¥é”™è¯¯")
                print(f"âŒ å¾®è°ƒè¯·æ±‚è¢«æ‹’ç»ï¼š{error_msg}")
                return False
            
            print("\nğŸ“Š å¾®è°ƒå®æ—¶æ—¥å¿—ï¼ˆæŒ‰Ctrl+Cå¯ä¸­æ–­ï¼‰ï¼š")
            for line in response.iter_lines():
                if line:
                    log_data = json.loads(line.decode("utf-8"))
                    # æ‰“å°è®­ç»ƒçŠ¶æ€ï¼ˆå¦‚epochã€lossã€è¿›åº¦ï¼‰
                    if "status" in log_data:
                        print(f"[{time.strftime('%H:%M:%S')}] {log_data['status']}")
                    # æ‰“å°è®­ç»ƒæŒ‡æ ‡ï¼ˆlossã€accuracyï¼Œéƒ¨åˆ†æ¨¡å‹æ”¯æŒï¼‰
                    if "metrics" in log_data:
                        metrics = log_data["metrics"]
                        print(f"[{time.strftime('%H:%M:%S')}] è®­ç»ƒæŒ‡æ ‡ï¼š{json.dumps(metrics, ensure_ascii=False)}")
                    # å¾®è°ƒå®Œæˆæ ‡å¿—
                    if "completed" in log_data and log_data["completed"]:
                        print(f"\nğŸ‰ æ¨¡å‹å¾®è°ƒå®Œæˆï¼å¾®è°ƒåæ¨¡å‹ï¼š{finetuned_model}")
                        print(f"   å¯é€šè¿‡å‘½ä»¤æŸ¥çœ‹ï¼šollama list | findstr {finetuned_model}")
                        return True
        print("âŒ å¾®è°ƒè¿‡ç¨‹æ„å¤–ä¸­æ–­")
        return False
    except KeyboardInterrupt:
        print(f"\nâš ï¸  å·²æ‰‹åŠ¨ä¸­æ–­å¾®è°ƒï¼Œæ¨¡å‹å¯èƒ½æœªä¿å­˜")
        return False
    except Exception as e:
        print(f"âŒ å¾®è°ƒæ—¶å‡ºé”™ï¼š{str(e)}")
        return False

def ollama_infer(model_name, prompt):
    """è°ƒç”¨å¾®è°ƒåçš„Ollamaæ¨¡å‹è¿›è¡Œæ¨ç†ï¼ˆé¢„æµ‹æµ‹è¯•é›†æ ‡ç­¾ï¼‰"""
    infer_payload = {
        "model": model_name,
        "prompt": prompt,
        "max_tokens": 10,  # ä»…éœ€è¾“å‡ºA/B/SAMEï¼Œé™åˆ¶ tokens é¿å…å†—ä½™
        "temperature": 0.1,  # ä½æ¸©åº¦=è¾“å‡ºæ›´ç¨³å®šï¼ˆé¿å…éšæœºç»“æœï¼‰
        "stream": False,  # éæµå¼å“åº”ï¼ˆç›´æ¥è·å–å®Œæ•´ç»“æœï¼‰
        "stop": ["\n"]  # é‡åˆ°æ¢è¡Œç¬¦åœæ­¢ï¼Œé¿å…å¤šä½™è¾“å‡º
    }
    
    try:
        response = requests.post(f"{OLLAMA_BASE_URL}/api/generate", json=infer_payload, timeout=15)
        if response.status_code != 200:
            error_msg = response.json().get("error", "æœªçŸ¥é”™è¯¯")
            print(f"âš ï¸ æ¨ç†å¤±è´¥ï¼š{error_msg}")
            return None
        # æå–æ¨¡å‹è¾“å‡ºï¼ˆOllamaè¿”å›çš„"response"å­—æ®µï¼‰
        return response.json().get("response", "").strip()
    except Exception as e:
        print(f"âš ï¸ æ¨ç†æ—¶å‡ºé”™ï¼š{str(e)}")
        return None

# -------------------------- 3. è¯„ä¼°ä¸å¯è§†åŒ–å‡½æ•° --------------------------
def parse_ollama_prediction(pred_text):
    """å°†Ollamaçš„è¾“å‡ºæ–‡æœ¬ï¼ˆA/B/SAMEï¼‰è§£æä¸ºæ•°å­—æ ‡ç­¾ï¼ˆ0/1/2ï¼‰"""
    if not pred_text:
        return -1  # æ— æ•ˆé¢„æµ‹ï¼ˆç©ºè¾“å‡ºï¼‰
    pred_text_upper = pred_text.strip().upper()
    # ç²¾ç¡®åŒ¹é…
    if pred_text_upper == "A":
        return 0
    elif pred_text_upper == "B":
        return 1
    elif pred_text_upper == "SAME":
        return 2
    # æ¨¡ç³ŠåŒ¹é…ï¼ˆåº”å¯¹æ¨¡å‹å¯èƒ½çš„å¤šä½™è¾“å‡ºï¼Œå¦‚"A "æˆ–"A\n"ï¼‰
    elif "A" in pred_text_upper and "B" not in pred_text_upper and "SAME" not in pred_text_upper:
        return 0
    elif "B" in pred_text_upper and "A" not in pred_text_upper and "SAME" not in pred_text_upper:
        return 1
    elif "SAME" in pred_text_upper or "TIE" in pred_text_upper:
        return 2
    # æ— æ³•è§£æçš„è¾“å‡º
    else:
        print(f"âš ï¸ æ— æ³•è§£æé¢„æµ‹ç»“æœï¼š{pred_text}ï¼Œæ ‡è®°ä¸ºæ— æ•ˆ")
        return -1

def calculate_evaluation_metrics(true_labels, pred_labels):
    """è®¡ç®—æ ¸å¿ƒè¯„ä¼°æŒ‡æ ‡ï¼šå‡†ç¡®ç‡ã€åŠ æƒF1ã€åˆ†ç±»æŠ¥å‘Šã€æ··æ·†çŸ©é˜µ"""
    # è¿‡æ»¤æ— æ•ˆé¢„æµ‹ï¼ˆpred_label=-1çš„æ ·æœ¬ï¼‰
    valid_mask = [p != -1 for p in pred_labels]
    valid_true = [t for t, v in zip(true_labels, valid_mask) if v]
    valid_pred = [p for p, v in zip(pred_labels, valid_mask) if v]
    
    if len(valid_true) == 0:
        raise ValueError("âŒ æ²¡æœ‰æœ‰æ•ˆé¢„æµ‹ç»“æœï¼Œæ— æ³•è®¡ç®—æŒ‡æ ‡ï¼Œè¯·æ£€æŸ¥æ¨¡å‹æ¨ç†è¿‡ç¨‹")
    
    # è®¡ç®—æŒ‡æ ‡
    accuracy = accuracy_score(valid_true, valid_pred)
    weighted_f1 = f1_score(valid_true, valid_pred, average="weighted")  # å¤šåˆ†ç±»ç”¨åŠ æƒF1
    class_report = classification_report(
        valid_true, valid_pred,
        target_names=LABEL_MAPPING.values(),
        digits=4  # ä¿ç•™4ä½å°æ•°ï¼Œç²¾åº¦æ›´é«˜
    )
    conf_matrix = confusion_matrix(valid_true, valid_pred)
    
    return {
        "valid_sample_count": len(valid_true),
        "total_sample_count": len(true_labels),
        "accuracy": accuracy,
        "weighted_f1": weighted_f1,
        "classification_report": class_report,
        "confusion_matrix": conf_matrix
    }

def plot_and_save_confusion_matrix(conf_matrix, save_path):
    """ç»˜åˆ¶æ··æ·†çŸ©é˜µçƒ­åŠ›å›¾å¹¶ä¿å­˜ï¼ˆç›´è§‚å±•ç¤ºåˆ†ç±»æ•ˆæœï¼‰"""
    plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']  # æ”¯æŒä¸­æ–‡æ˜¾ç¤º
    plt.figure(figsize=(8, 6))
    
    # ç»˜åˆ¶çƒ­åŠ›å›¾
    sns.heatmap(
        conf_matrix,
        annot=True,  # æ˜¾ç¤ºæ•°å€¼
        fmt="d",  # æ•°å€¼æ ¼å¼ï¼ˆæ•´æ•°ï¼‰
        cmap="Blues",  # é¢œè‰²æ˜ å°„
        xticklabels=LABEL_MAPPING.values(),  # xè½´æ ‡ç­¾ï¼ˆé¢„æµ‹æ ‡ç­¾ï¼‰
        yticklabels=LABEL_MAPPING.values(),  # yè½´æ ‡ç­¾ï¼ˆçœŸå®æ ‡ç­¾ï¼‰
        cbar_kws={"label": "æ ·æœ¬æ•°é‡"}  # é¢œè‰²æ¡æ ‡ç­¾
    )
    
    # è®¾ç½®å›¾è¡¨æ ‡é¢˜å’Œè½´æ ‡ç­¾
    plt.title("æ¨¡å‹æ··æ·†çŸ©é˜µ", fontsize=14, pad=20)
    plt.xlabel("é¢„æµ‹æ ‡ç­¾", fontsize=12, labelpad=10)
    plt.ylabel("çœŸå®æ ‡ç­¾", fontsize=12, labelpad=10)
    plt.tight_layout()  # è‡ªåŠ¨è°ƒæ•´å¸ƒå±€ï¼Œé¿å…æ ‡ç­¾è¢«æˆªæ–­
    
    # ä¿å­˜å›¾ç‰‡
    plt.savefig(save_path, dpi=300, bbox_inches="tight")  # dpi=300ä¿è¯é«˜æ¸…
    plt.close()
    print(f"âœ… æ··æ·†çŸ©é˜µå·²ä¿å­˜åˆ°ï¼š{save_path}")

def save_evaluation_results(results, metrics, save_dir):
    """ä¿å­˜æ¨ç†ç»“æœå’Œè¯„ä¼°æŒ‡æ ‡åˆ°æ–‡ä»¶ï¼ˆä¾¿äºåç»­åˆ†æï¼‰"""
    # 1. ä¿å­˜æ¨ç†ç»“æœï¼ˆCSVæ ¼å¼ï¼ŒåŒ…å«çœŸå®æ ‡ç­¾ã€é¢„æµ‹æ–‡æœ¬ã€é¢„æµ‹æ ‡ç­¾ï¼‰
    results_df = pd.DataFrame(results)
    results_csv_path = os.path.join(save_dir, "test_inference_results.csv")
    results_df.to_csv(results_csv_path, index=False, encoding="utf-8-sig")
    print(f"âœ… æ¨ç†ç»“æœå·²ä¿å­˜åˆ°ï¼š{results_csv_path}")
    
    # 2. ä¿å­˜è¯„ä¼°æŒ‡æ ‡ï¼ˆTXTæ ¼å¼ï¼ŒåŒ…å«å‡†ç¡®ç‡ã€F1ã€åˆ†ç±»æŠ¥å‘Šï¼‰
    metrics_txt_path = os.path.join(save_dir, "evaluation_metrics.txt")
    with open(metrics_txtimport argparse
import os
import json
import time
import pandas as pd
import numpy as np
import requests
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, f1_score
import matplotlib.pyplot as plt
import seaborn as sns

# -------------------------- æœ¬åœ°Ollamaæ ¸å¿ƒé…ç½®ï¼ˆéœ€æ ¹æ®ä½ çš„ç¯å¢ƒç¡®è®¤ï¼‰ --------------------------
OLLAMA_BASE_URL = "http://127.0.0.1:11434"  # Ollamaé»˜è®¤æœ¬åœ°æœåŠ¡ç«¯å£ï¼ˆå›ºå®šï¼‰
OLLAMA_BASE_MODEL = "llama3:8b"  # æœ¬åœ°åŸºç¡€æ¨¡å‹ï¼ˆéœ€å…ˆé€šè¿‡ `ollama pull llama3:8b` ä¸‹è½½ï¼‰
FINETUNED_MODEL_NAME = "llama3:8b-model-compare-finetuned"  # å¾®è°ƒåæ¨¡å‹çš„è‡ªå®šä¹‰åç§°
DATA_FILE = "train.csv"  # æœ¬åœ°æ•°æ®æ–‡ä»¶ï¼ˆéœ€ä¸ä»£ç åŒç›®å½•ï¼Œæˆ–å†™ç»å¯¹è·¯å¾„ï¼‰
LABEL_MAPPING = {0: "Aæ›´å¥½", 1: "Bæ›´å¥½", 2: "ä¸€æ ·å¥½"}  # æ ‡ç­¾æ–‡æœ¬æ˜ å°„
LABEL_TO_OLLAMA = {0: "A", 1: "B", 2: "SAME"}  # å¾®è°ƒæ—¶ä¼ ç»™Ollamaçš„ç®€æ´æ ‡ç­¾

# -------------------------- 1. æ•°æ®å¤„ç†å·¥å…·å‡½æ•° --------------------------
def read_data(path):
    """è¯»å–CSV/JSONLæ•°æ®ï¼Œå…¼å®¹ä¸¤ç§æ ¼å¼"""
    if path.endswith(".csv"):
        return pd.read_csv(path, encoding="utf-8")
    if path.endswith(".jsonl") or path.endswith(".json"):
        return pd.read_json(path, lines=True, encoding="utf-8")
    raise ValueError("ä»…æ”¯æŒ CSV æˆ– JSONL/JSON æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥æ•°æ®æ ¼å¼")

def truthy(value):
    """åˆ¤æ–­å€¼æ˜¯å¦ä¸ºâ€œçœŸâ€ï¼ˆç”¨äºè§£ææ ‡æ³¨çš„winneræ ‡ç­¾ï¼‰"""
    if pd.isna(value):
        return False
    if isinstance(value, (int, float, np.integer, np.floating)):
        return int(value) != 0
    value_str = str(value).strip().lower()
    return value_str in ("1", "true", "yes", "y", "t", "1.0")

def infer_true_label(row):
    """ä»æ•°æ®è¡Œä¸­æå–çœŸå®æ ‡ç­¾ï¼ˆ0=Aæ›´å¥½ï¼Œ1=Bæ›´å¥½ï¼Œ2=ä¸€æ ·å¥½ï¼‰"""
    # ä¼˜å…ˆè§£æå¤šåˆ—æ ‡æ³¨ï¼ˆwinner_model_a/winner_model_b/winner_tieï¼‰
    if truthy(row.get("winner_model_a", None)):
        return 0
    if truthy(row.get("winner_model_b", None)):
        return 1
    if truthy(row.get("winner_tie", None)):
        return 2
    # å…¼å®¹å•åˆ—æ ‡æ³¨ï¼ˆä»…winneråˆ—ï¼‰
    winner_col = row.get("winner", None)
    if pd.notna(winner_col):
        winner_str = str(winner_col).strip().lower()
        if winner_str in ("a", "model_a", "model a", "winner a"):
            return 0
        if winner_str in ("b", "model_b", "model b", "winner b"):
            return 1
        if winner_str in ("tie", "same", "equal", "both"):
            return 2
    # è‹¥æ— æ³•è§£æï¼ŒæŠ›å‡ºé”™è¯¯ï¼ˆéœ€æ£€æŸ¥æ•°æ®æ ‡æ³¨æ ¼å¼ï¼‰
    raise ValueError(f"æ— æ³•è§£ææ ‡ç­¾ï¼è¡ŒIDï¼š{row.get('id', 'æœªçŸ¥')}ï¼Œè¯·æ£€æŸ¥æ ‡æ³¨åˆ—")

def build_ollama_finetune_sample(row):
    """æ„å»ºOllamaå¾®è°ƒè¦æ±‚çš„æ ·æœ¬æ ¼å¼ï¼ˆJSONLï¼š{"prompt": "...", "response": "..."}ï¼‰"""
    prompt = str(row.get("prompt", "")).strip()
    response_a = str(row.get("response_a", "")).strip()
    response_b = str(row.get("response_b", "")).strip()
    true_label = row["true_label"]  # å·²é€šè¿‡infer_true_labelè·å–
    
    # å¾®è°ƒæç¤ºè¯ï¼ˆæ˜ç¡®ä»»åŠ¡ï¼Œè®©æ¨¡å‹å­¦ä¹ åˆ¤æ–­é€»è¾‘ï¼‰
    finetune_prompt = f"""ä»»åŠ¡ï¼šå¯¹æ¯”ä¸¤ä¸ªæ¨¡å‹çš„å›ç­”ï¼Œåˆ¤æ–­å“ªä¸ªæ›´ç¬¦åˆç”¨æˆ·æç¤ºã€‚
ç”¨æˆ·æç¤ºï¼š{prompt}
æ¨¡å‹Açš„å›ç­”ï¼š{response_a}
æ¨¡å‹Bçš„å›ç­”ï¼š{response_b}
è¦æ±‚ï¼šä»…è¾“å‡ºæœ€ç»ˆåˆ¤æ–­ç»“æœï¼ˆA=Aæ›´å¥½ï¼ŒB=Bæ›´å¥½ï¼ŒSAME=ä¸¤è€…ä¸€æ ·å¥½ï¼‰ï¼Œä¸è¦é¢å¤–æ–‡å­—ã€‚
åˆ¤æ–­ç»“æœï¼š"""
    
    # Ollamaå¾®è°ƒè¦æ±‚çš„é”®å¿…é¡»æ˜¯"prompt"å’Œ"response"
    return {
        "prompt": finetune_prompt,
        "response": LABEL_TO_OLLAMA[true_label]
    }

def save_finetune_data(samples, save_path):
    """å°†å¾®è°ƒæ ·æœ¬ä¿å­˜ä¸ºOllamaæ”¯æŒçš„JSONLæ ¼å¼"""
    with open(save_path, "w", encoding="utf-8") as f:
        for sample in samples:
            f.write(json.dumps(sample, ensure_ascii=False) + "\n")
    print(f"âœ… å¾®è°ƒæ•°æ®å·²ä¿å­˜åˆ°ï¼š{save_path}")
    return save_path

# -------------------------- 2. OllamaæœåŠ¡äº¤äº’å‡½æ•° --------------------------
def check_ollama_service():
    """æ£€æŸ¥æœ¬åœ°OllamaæœåŠ¡æ˜¯å¦å·²å¯åŠ¨ï¼ˆå¿…é¡»å…ˆå¯åŠ¨æœåŠ¡æ‰èƒ½è¿è¡Œï¼‰"""
    try:
        response = requests.get(f"{OLLAMA_BASE_URL}/api/tags", timeout=5)
        if response.status_code == 200:
            print("âœ… æœ¬åœ°OllamaæœåŠ¡å·²æ­£å¸¸å¯åŠ¨")
            return True
        else:
            print(f"âŒ OllamaæœåŠ¡å“åº”å¼‚å¸¸ï¼ŒçŠ¶æ€ç ï¼š{response.status_code}")
            return False
    except requests.exceptions.ConnectionError:
        print("âŒ æœªæ£€æµ‹åˆ°OllamaæœåŠ¡ï¼è¯·å…ˆå¯åŠ¨æœåŠ¡ï¼š")
        print("  1. æ‰“å¼€CMDå‘½ä»¤æç¤ºç¬¦")
        print("  2. è¾“å…¥å‘½ä»¤ï¼šollama serveï¼ˆä¸è¦å…³é—­æ­¤CMDçª—å£ï¼‰")
        print("  3. é‡æ–°è¿è¡Œæœ¬ä»£ç ")
        return False

def pull_ollama_base_model(model_name):
    """æ‹‰å–åŸºç¡€æ¨¡å‹ï¼ˆå¦‚æœæœ¬åœ°æ²¡æœ‰ï¼‰ï¼Œç¡®ä¿å¾®è°ƒèƒ½æ­£å¸¸è¿›è¡Œ"""
    print(f"\nğŸ” æ£€æŸ¥æœ¬åœ°æ˜¯å¦å­˜åœ¨åŸºç¡€æ¨¡å‹ï¼š{model_name}")
    # å…ˆæŸ¥è¯¢æœ¬åœ°å·²æœ‰çš„æ¨¡å‹
    try:
        response = requests.get(f"{OLLAMA_BASE_URL}/api/tags", timeout=10)
        local_models = [m["name"] for m in response.json().get("models", [])]
        if model_name in local_models:
            print(f"âœ… åŸºç¡€æ¨¡å‹ {model_name} å·²å­˜åœ¨äºæœ¬åœ°")
            return True
    except Exception as e:
        print(f"âš ï¸ æŸ¥è¯¢æœ¬åœ°æ¨¡å‹æ—¶å‡ºé”™ï¼š{str(e)}ï¼Œå°†å°è¯•ç›´æ¥æ‹‰å–")
    
    # æœ¬åœ°æ²¡æœ‰æ¨¡å‹ï¼Œå¼€å§‹æ‹‰å–ï¼ˆå¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼Œå–å†³äºç½‘ç»œå’Œæ¨¡å‹å¤§å°ï¼‰
    print(f"ğŸ“¥ å¼€å§‹æ‹‰å–åŸºç¡€æ¨¡å‹ {model_name}ï¼ˆè¯·å‹¿ä¸­æ–­ï¼‰...")
    payload = {"name": model_name}
    try:
        # Ollamaæ‹‰å–æ˜¯æµå¼å“åº”ï¼Œå®æ—¶æ‰“å°è¿›åº¦
        with requests.post(
            f"{OLLAMA_BASE_URL}/api/pull", 
            json=payload, 
            stream=True, 
            timeout=3600  # è¶…æ—¶è®¾ä¸º1å°æ—¶ï¼Œé¿å…å¤§æ¨¡å‹æ‹‰å–ä¸­æ–­
        ) as response:
            for line in response.iter_lines():
                if line:
                    data = json.loads(line.decode("utf-8"))
                    # æ‰“å°å…³é”®çŠ¶æ€ï¼ˆå¦‚ä¸‹è½½è¿›åº¦ã€è§£å‹çŠ¶æ€ï¼‰
                    if "status" in data:
                        print(f"  æ‹‰å–çŠ¶æ€ï¼š{data['status']}", end="\r")
                    # æ‹‰å–å®Œæˆçš„æ ‡å¿—
                    if "completed" in data and data["completed"]:
                        print(f"\nâœ… åŸºç¡€æ¨¡å‹ {model_name} æ‹‰å–å®Œæˆ")
                        return True
        print(f"âŒ æ¨¡å‹æ‹‰å–æœªå®Œæˆï¼ˆæœªçŸ¥åŸå› ï¼‰")
        return False
    except Exception as e:
        print(f"âŒ æ‹‰å–æ¨¡å‹æ—¶å‡ºé”™ï¼š{str(e)}ï¼Œè¯·æ£€æŸ¥ç½‘ç»œåé‡è¯•")
        return False

def finetune_with_ollama(base_model, finetuned_model, data_path):
    """è°ƒç”¨Ollama APIè¿›è¡Œæ¨¡å‹å¾®è°ƒï¼ˆæ ¸å¿ƒæ­¥éª¤ï¼Œä½¿ç”¨æœ¬åœ°GPUåŠ é€Ÿï¼‰"""
    print(f"\nğŸš€ å¼€å§‹å¾®è°ƒæ¨¡å‹ï¼š")
    print(f"  åŸºç¡€æ¨¡å‹ï¼š{base_model}")
    print(f"  å¾®è°ƒåæ¨¡å‹åï¼š{finetuned_model}")
    print(f"  å¾®è°ƒæ•°æ®è·¯å¾„ï¼š{data_path}")
    
    # Ollamaå¾®è°ƒå‚æ•°ï¼ˆæ ¹æ®GPUå†…å­˜è°ƒæ•´ï¼Œå†…å­˜å°åˆ™å‡å°batch_sizeï¼‰
    finetune_payload = {
        "base_model": base_model,
        "name": finetuned_model,
        "data": data_path,
        "options": {
            "gpu": True,  # å¼ºåˆ¶å¯ç”¨GPUåŠ é€Ÿï¼ˆOllamaä¼šè‡ªåŠ¨è¯†åˆ«æœ¬åœ°GPUï¼‰
            "batch_size": 2,  # æ‰¹æ¬¡å¤§å°ï¼ˆ16GBå†…å­˜å»ºè®®2-4ï¼Œ32GBå»ºè®®4-8ï¼‰
            "epochs": 3,  # è®­ç»ƒè½®æ•°ï¼ˆæ•°æ®é‡å°æ—¶å¯å¢åŠ åˆ°5ï¼Œé¿å…è¿‡æ‹Ÿåˆï¼‰
            "learning_rate": 1e-4  # å­¦ä¹ ç‡ï¼ˆOllamaé»˜è®¤1e-4ï¼Œæ— éœ€é¢‘ç¹ä¿®æ”¹ï¼‰
        }
    }
    
    try:
        # å‘é€å¾®è°ƒè¯·æ±‚ï¼ˆæµå¼å“åº”ï¼Œå®æ—¶æ‰“å°è®­ç»ƒæ—¥å¿—ï¼‰
        with requests.post(
            f"{OLLAMA_BASE_URL}/api/finetune", 
            json=finetune_payload, 
            stream=True, 
            timeout=3600  # å¾®è°ƒè¶…æ—¶è®¾ä¸º1å°æ—¶
        ) as response:
            if response.status_code != 200:
                error_msg = response.json().get("error", "æœªçŸ¥é”™è¯¯")
                print(f"âŒ å¾®è°ƒè¯·æ±‚è¢«æ‹’ç»ï¼š{error_msg}")
                return False
            
            print("\nğŸ“Š å¾®è°ƒå®æ—¶æ—¥å¿—ï¼ˆæŒ‰Ctrl+Cå¯ä¸­æ–­ï¼‰ï¼š")
            for line in response.iter_lines():
                if line:
                    log_data = json.loads(line.decode("utf-8"))
                    # æ‰“å°è®­ç»ƒçŠ¶æ€ï¼ˆå¦‚epochã€lossã€è¿›åº¦ï¼‰
                    if "status" in log_data:
                        print(f"[{time.strftime('%H:%M:%S')}] {log_data['status']}")
                    # æ‰“å°è®­ç»ƒæŒ‡æ ‡ï¼ˆlossã€accuracyï¼Œéƒ¨åˆ†æ¨¡å‹æ”¯æŒï¼‰
                    if "metrics" in log_data:
                        metrics = log_data["metrics"]
                        print(f"[{time.strftime('%H:%M:%S')}] è®­ç»ƒæŒ‡æ ‡ï¼š{json.dumps(metrics, ensure_ascii=False)}")
                    # å¾®è°ƒå®Œæˆæ ‡å¿—
                    if "completed" in log_data and log_data["completed"]:
                        print(f"\nğŸ‰ æ¨¡å‹å¾®è°ƒå®Œæˆï¼å¾®è°ƒåæ¨¡å‹ï¼š{finetuned_model}")
                        print(f"   å¯é€šè¿‡å‘½ä»¤æŸ¥çœ‹ï¼šollama list | findstr {finetuned_model}")
                        return True
        print("âŒ å¾®è°ƒè¿‡ç¨‹æ„å¤–ä¸­æ–­")
        return False
    except KeyboardInterrupt:
        print(f"\nâš ï¸  å·²æ‰‹åŠ¨ä¸­æ–­å¾®è°ƒï¼Œæ¨¡å‹å¯èƒ½æœªä¿å­˜")
        return False
    except Exception as e:
        print(f"âŒ å¾®è°ƒæ—¶å‡ºé”™ï¼š{str(e)}")
        return False

def ollama_infer(model_name, prompt):
    """è°ƒç”¨å¾®è°ƒåçš„Ollamaæ¨¡å‹è¿›è¡Œæ¨ç†ï¼ˆé¢„æµ‹æµ‹è¯•é›†æ ‡ç­¾ï¼‰"""
    infer_payload = {
        "model": model_name,
        "prompt": prompt,
        "max_tokens": 10,  # ä»…éœ€è¾“å‡ºA/B/SAMEï¼Œé™åˆ¶ tokens é¿å…å†—ä½™
        "temperature": 0.1,  # ä½æ¸©åº¦=è¾“å‡ºæ›´ç¨³å®šï¼ˆé¿å…éšæœºç»“æœï¼‰
        "stream": False,  # éæµå¼å“åº”ï¼ˆç›´æ¥è·å–å®Œæ•´ç»“æœï¼‰
        "stop": ["\n"]  # é‡åˆ°æ¢è¡Œç¬¦åœæ­¢ï¼Œé¿å…å¤šä½™è¾“å‡º
    }
    
    try:
        response = requests.post(f"{OLLAMA_BASE_URL}/api/generate", json=infer_payload, timeout=15)
        if response.status_code != 200:
            error_msg = response.json().get("error", "æœªçŸ¥é”™è¯¯")
            print(f"âš ï¸ æ¨ç†å¤±è´¥ï¼š{error_msg}")
            return None
        # æå–æ¨¡å‹è¾“å‡ºï¼ˆOllamaè¿”å›çš„"response"å­—æ®µï¼‰
        return response.json().get("response", "").strip()
    except Exception as e:
        print(f"âš ï¸ æ¨ç†æ—¶å‡ºé”™ï¼š{str(e)}")
        return None

# -------------------------- 3. è¯„ä¼°ä¸å¯è§†åŒ–å‡½æ•° --------------------------
def parse_ollama_prediction(pred_text):
    """å°†Ollamaçš„è¾“å‡ºæ–‡æœ¬ï¼ˆA/B/SAMEï¼‰è§£æä¸ºæ•°å­—æ ‡ç­¾ï¼ˆ0/1/2ï¼‰"""
    if not pred_text:
        return -1  # æ— æ•ˆé¢„æµ‹ï¼ˆç©ºè¾“å‡ºï¼‰
    pred_text_upper = pred_text.strip().upper()
    # ç²¾ç¡®åŒ¹é…
    if pred_text_upper == "A":
        return 0
    elif pred_text_upper == "B":
        return 1
    elif pred_text_upper == "SAME":
        return 2
    # æ¨¡ç³ŠåŒ¹é…ï¼ˆåº”å¯¹æ¨¡å‹å¯èƒ½çš„å¤šä½™è¾“å‡ºï¼Œå¦‚"A "æˆ–"A\n"ï¼‰
    elif "A" in pred_text_upper and "B" not in pred_text_upper and "SAME" not in pred_text_upper:
        return 0
    elif "B" in pred_text_upper and "A" not in pred_text_upper and "SAME" not in pred_text_upper:
        return 1
    elif "SAME" in pred_text_upper or "TIE" in pred_text_upper:
        return 2
    # æ— æ³•è§£æçš„è¾“å‡º
    else:
        print(f"âš ï¸ æ— æ³•è§£æé¢„æµ‹ç»“æœï¼š{pred_text}ï¼Œæ ‡è®°ä¸ºæ— æ•ˆ")
        return -1

def calculate_evaluation_metrics(true_labels, pred_labels):
    """è®¡ç®—æ ¸å¿ƒè¯„ä¼°æŒ‡æ ‡ï¼šå‡†ç¡®ç‡ã€åŠ æƒF1ã€åˆ†ç±»æŠ¥å‘Šã€æ··æ·†çŸ©é˜µ"""
    # è¿‡æ»¤æ— æ•ˆé¢„æµ‹ï¼ˆpred_label=-1çš„æ ·æœ¬ï¼‰
    valid_mask = [p != -1 for p in pred_labels]
    valid_true = [t for t, v in zip(true_labels, valid_mask) if v]
    valid_pred = [p for p, v in zip(pred_labels, valid_mask) if v]
    
    if len(valid_true) == 0:
        raise ValueError("âŒ æ²¡æœ‰æœ‰æ•ˆé¢„æµ‹ç»“æœï¼Œæ— æ³•è®¡ç®—æŒ‡æ ‡ï¼Œè¯·æ£€æŸ¥æ¨¡å‹æ¨ç†è¿‡ç¨‹")
    
    # è®¡ç®—æŒ‡æ ‡
    accuracy = accuracy_score(valid_true, valid_pred)
    weighted_f1 = f1_score(valid_true, valid_pred, average="weighted")  # å¤šåˆ†ç±»ç”¨åŠ æƒF1
    class_report = classification_report(
        valid_true, valid_pred,
        target_names=LABEL_MAPPING.values(),
        digits=4  # ä¿ç•™4ä½å°æ•°ï¼Œç²¾åº¦æ›´é«˜
    )
    conf_matrix = confusion_matrix(valid_true, valid_pred)
    
    return {
        "valid_sample_count": len(valid_true),
        "total_sample_count": len(true_labels),
        "accuracy": accuracy,
        "weighted_f1": weighted_f1,
        "classification_report": class_report,
        "confusion_matrix": conf_matrix
    }

def plot_and_save_confusion_matrix(conf_matrix, save_path):
    """ç»˜åˆ¶æ··æ·†çŸ©é˜µçƒ­åŠ›å›¾å¹¶ä¿å­˜ï¼ˆç›´è§‚å±•ç¤ºåˆ†ç±»æ•ˆæœï¼‰"""
    plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']  # æ”¯æŒä¸­æ–‡æ˜¾ç¤º
    plt.figure(figsize=(8, 6))
    
    # ç»˜åˆ¶çƒ­åŠ›å›¾
    sns.heatmap(
        conf_matrix,
        annot=True,  # æ˜¾ç¤ºæ•°å€¼
        fmt="d",  # æ•°å€¼æ ¼å¼ï¼ˆæ•´æ•°ï¼‰
        cmap="Blues",  # é¢œè‰²æ˜ å°„
        xticklabels=LABEL_MAPPING.values(),  # xè½´æ ‡ç­¾ï¼ˆé¢„æµ‹æ ‡ç­¾ï¼‰
        yticklabels=LABEL_MAPPING.values(),  # yè½´æ ‡ç­¾ï¼ˆçœŸå®æ ‡ç­¾ï¼‰
        cbar_kws={"label": "æ ·æœ¬æ•°é‡"}  # é¢œè‰²æ¡æ ‡ç­¾
    )
    
    # è®¾ç½®å›¾è¡¨æ ‡é¢˜å’Œè½´æ ‡ç­¾
    plt.title("æ¨¡å‹æ··æ·†çŸ©é˜µ", fontsize=14, pad=20)
    plt.xlabel("é¢„æµ‹æ ‡ç­¾", fontsize=12, labelpad=10)
    plt.ylabel("çœŸå®æ ‡ç­¾", fontsize=12, labelpad=10)
    plt.tight_layout()  # è‡ªåŠ¨è°ƒæ•´å¸ƒå±€ï¼Œé¿å…æ ‡ç­¾è¢«æˆªæ–­
    
    # ä¿å­˜å›¾ç‰‡
    plt.savefig(save_path, dpi=300, bbox_inches="tight")  # dpi=300ä¿è¯é«˜æ¸…
    plt.close()
    print(f"âœ… æ··æ·†çŸ©é˜µå·²ä¿å­˜åˆ°ï¼š{save_path}")

def save_evaluation_results(results, metrics, save_dir):
    """ä¿å­˜æ¨ç†ç»“æœå’Œè¯„ä¼°æŒ‡æ ‡åˆ°æ–‡ä»¶ï¼ˆä¾¿äºåç»­åˆ†æï¼‰"""
    # 1. ä¿å­˜æ¨ç†ç»“æœï¼ˆCSVæ ¼å¼ï¼ŒåŒ…å«çœŸå®æ ‡ç­¾ã€é¢„æµ‹æ–‡æœ¬ã€é¢„æµ‹æ ‡ç­¾ï¼‰
    results_df = pd.DataFrame(results)
    results_csv_path = os.path.join(save_dir, "test_inference_results.csv")
    results_df.to_csv(results_csv_path, index=False, encoding="utf-8-sig")
    print(f"âœ… æ¨ç†ç»“æœå·²ä¿å­˜åˆ°ï¼š{results_csv_path}")
    
    # 2. ä¿å­˜è¯„ä¼°æŒ‡æ ‡ï¼ˆTXTæ ¼å¼ï¼ŒåŒ…å«å‡†ç¡®ç‡ã€F1ã€åˆ†ç±»æŠ¥å‘Šï¼‰
    metrics_txt_path = os.path.join(save_dir, "evaluation_metrics.txt")
    with open(metrics_txt